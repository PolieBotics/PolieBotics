#### **Optical Computation Using Projector-Camera Setups**

We present an *optical pipeline* for core linear operations in machine learning—particularly convolution, fully connected layers, and frequency transforms—achieved via **light-based interactions** rather than digital multiply-accumulate. A high-resolution projector and synchronized camera (optionally within an optical resonance cavity) can replace standard CPU/GPU kernels by exploiting physical summation of projected intensities.

1. **Analogue Convolution**: A *2160p projector* can be “rasterized” to convolve a *1080p image* with a 4×4 kernel by grouping each 4×4 block of the projector to represent a single lower-res pixel. When the projector outputs kernel-weighted intensities, the camera captures an *optical sum* equivalent to the convolution result. This avoids large-scale digital multiply-accumulate, offloading them to photonic summation in free space.
2. **Fully Connected Layers**: An 8×88\times8 input layer with 64 neurons can be implemented by **64 micro-projectors** (or scanning positions), each encoding a neuron’s weight distribution. When projected in parallel (or sequentially via time division), the camera integrates the contributions from all weights in one optical capture. Alternatively, a single high-resolution *micro-LED projector* with global shutter can simultaneously emit all neurons’ patterns, capturing the final weighted output in a single shot.
3. **Frequency & Wavelet Transforms**: Projecting FFT or wavelet-transformed images into an *optical resonance cavity* enables partial filtering, compression, or feature extraction at the speed of light. The cavity selectively amplifies/suppresses specific frequency or wavelet coefficients, recorded by the camera without repeated digital transforms.
4. **Multiplexing & Downsampling**: Multiple signals can be combined via **time division** (rapid sequential projection), **frequency division** (distinct color channels), or **partial spatial** multiplexing (subsections of the environment). Downsampling from 2160p to 1080p similarly arises by *averaging* (physically adding) each 2×2 pixel block in the projector domain, captured by the camera as a single lower-resolution pixel.

By embedding these optical operations into traditional ML pipelines, we exploit inherent photonic parallelism. Convolution, matrix multiplication, and transform-domain filtering become *light-based computations*, reducing electronic overhead and offering new directions for high-throughput, energy-efficient model inference.